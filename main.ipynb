{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import random\n",
    "\n",
    "conn = sqlite3.connect('Data/data.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "query = c.execute(\"SELECT author || '-' || title, text FROM lyrics\")\n",
    "\n",
    "lyrics = []\n",
    "\n",
    "for row in query:\n",
    "    lyrics.append(row)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "random.shuffle(lyrics)\n",
    "\n",
    "lyrics = lyrics[0:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lyrics_train) = 7000\n",
      "len(lyrics_test) = 3000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lyrics_train, lyrics_test = train_test_split(lyrics, test_size=0.30)\n",
    "\n",
    "del lyrics\n",
    "\n",
    "print(\"len(lyrics_train) = %d\" % len(lyrics_train))\n",
    "print(\"len(lyrics_test) = %d\" % len(lyrics_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TOKENS = 4\n",
    "N_HASHES = 100\n",
    "N_BANDS = 10\n",
    "N_BUCKETS = 2**64\n",
    "KEY = 1\n",
    "TRUTH_KEY = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "import binascii\n",
    "import numpy as np\n",
    "\n",
    "def to_shingles(lyrics, n_tokens, key, silent=False):\n",
    "    shingles = []\n",
    "\n",
    "    for i, lyric in enumerate(lyrics):\n",
    "        features = list(ngrams(list(lyric[key]), n_tokens))\n",
    "        # Hashing\n",
    "        shingle = [binascii.crc32(\" \".join(feature)) & 0xffffffff for feature in features]\n",
    "        \n",
    "        if len(shingle) == 0:\n",
    "            print(\"WARNING: skipping empty shingle (i = %d)\" % i)\n",
    "            continue\n",
    "        \n",
    "        shingle.sort()\n",
    "        shingle = np.array(shingle, dtype=np.int)\n",
    "        shingles.append(shingle)\n",
    "\n",
    "        if not silent and i > 0 and i % (len(lyrics) / 10) == 0:\n",
    "            print(\"%d of %d\" % (i, len(lyrics)))\n",
    "\n",
    "    if not silent:\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 of 3000\n",
      "600 of 3000\n",
      "900 of 3000\n",
      "1200 of 3000\n",
      "1500 of 3000\n",
      "1800 of 3000\n",
      "2100 of 3000\n",
      "2400 of 3000\n",
      "2700 of 3000\n",
      "Done!\n",
      "CPU times: user 4.44 s, sys: 230 ms, total: 4.67 s\n",
      "Wall time: 4.33 s\n"
     ]
    }
   ],
   "source": [
    "%time shingles = to_shingles(lyrics_test, n_tokens=N_TOKENS, key=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Min Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_signatures(shingles, n_hashes, silent=False):\n",
    "    M = 4294967311\n",
    "\n",
    "    a = np.random.randint(2 ** 32, size=(n_hashes, 1)) # n_hashes x 1\n",
    "    b = np.random.randint(2 ** 32, size=(n_hashes, 1)) # 1 x n_hashes\n",
    "    \n",
    "    signatures = np.zeros((len(shingles), n_hashes), dtype=np.int) # len(shingles) x n_hashes\n",
    "\n",
    "    for i, shingle in enumerate(shingles):\n",
    "        tmp = (a * shingle + b) % M # n_hashes x len(shingle)\n",
    "        signature = np.min(tmp, axis=1)\n",
    "        signatures[i] = signature\n",
    "\n",
    "        if not silent and i > 0 and i % (len(shingles) / 10) == 0:\n",
    "            print(\"%d of %d\" % (i, len(shingles)))\n",
    "\n",
    "    if not silent:\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    return signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 of 3000\n",
      "600 of 3000\n",
      "900 of 3000\n",
      "1200 of 3000\n",
      "1500 of 3000\n",
      "1800 of 3000\n",
      "2100 of 3000\n",
      "2400 of 3000\n",
      "2700 of 3000\n",
      "Done!\n",
      "CPU times: user 5.91 s, sys: 7.69 ms, total: 5.92 s\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%time signatures = to_signatures(shingles, n_hashes=N_HASHES)\n",
    "\n",
    "del shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_h(M):\n",
    "    a = random.randint(0, M - 1)\n",
    "    b = random.randint(0, M - 1)\n",
    "\n",
    "    return lambda x: (a * x + b) % M\n",
    "\n",
    "def signature_hash(signature, start, end, n_buckets):\n",
    "    M = 4294967311\n",
    "    k = 0\n",
    "\n",
    "    for e in signature[start:end]:\n",
    "        k = (k + int(e) * M) % n_buckets\n",
    "    \n",
    "    return k\n",
    "\n",
    "def lsh(signatures, n_bands, n_buckets, silent=False):\n",
    "    rows = len(signatures[0]) / n_bands # rows per bucket\n",
    "\n",
    "    buckets = []\n",
    "    \n",
    "    for i in xrange(n_bands):\n",
    "        h = generate_h(n_buckets) # for each band, create a hash function\n",
    "        \n",
    "        band_buckets = {}\n",
    "        \n",
    "        start = rows * i\n",
    "\n",
    "        if i == n_bands - 1:\n",
    "            end = len(signature)\n",
    "        else:\n",
    "            end = rows * (i + 1)\n",
    "        \n",
    "        x = np.random.randn(1, len(signatures[0][start:end]))\n",
    "        b = np.random.random()\n",
    "    \n",
    "        for j, signature in enumerate(signatures):\n",
    "            k = signature_hash(signature, start, end, n_buckets)\n",
    "            \n",
    "            if k not in band_buckets:\n",
    "                band_buckets[k] = []\n",
    "            \n",
    "            band_buckets[k].append(j)\n",
    "        \n",
    "        for band_bucket in band_buckets.values():\n",
    "            if len(band_bucket) <= 1:\n",
    "                continue\n",
    "            \n",
    "            buckets.append(np.array(band_bucket, dtype=np.int))\n",
    "        \n",
    "        if not silent:\n",
    "            print(\"%d of %d\" % (i, n_bands))\n",
    "    \n",
    "    if not silent:\n",
    "        print(\"Done!\")\n",
    "        print(\"Average bucket length = %.2lf\" % np.mean([len(bucket) for bucket in buckets if len(bucket) > 0]))\n",
    "        print(\"Median bucket length = %.2lf\" % np.median([len(bucket) for bucket in buckets if len(bucket) > 0]))\n",
    "        print(\"Max bucket length = %.2lf\" % np.max([len(bucket) for bucket in buckets]))\n",
    "    \n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 10\n",
      "1 of 10\n",
      "2 of 10\n",
      "3 of 10\n",
      "4 of 10\n",
      "5 of 10\n",
      "6 of 10\n",
      "7 of 10\n",
      "8 of 10\n",
      "9 of 10\n",
      "Done!\n",
      "Average bucket length = 2.28\n",
      "Median bucket length = 2.00\n",
      "Max bucket length = 10.00\n",
      "CPU times: user 209 ms, sys: 21.1 ms, total: 230 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%time buckets = lsh(signatures, n_bands=N_BANDS, n_buckets=N_BUCKETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Duplicates with LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_false_negatives(lyrics, key):\n",
    "    false_negatives = {}\n",
    "\n",
    "    for row in lyrics:\n",
    "        false_negatives[row[key]] = false_negatives.get(row[key], 0) + 1\n",
    "    \n",
    "    false_negatives_keys = false_negatives.keys()\n",
    "\n",
    "    for k in false_negatives_keys:\n",
    "        false_negatives[k] = (false_negatives[k] * (false_negatives[k] - 1)) / 2\n",
    "        \n",
    "        if false_negatives[k] == 0:\n",
    "            del false_negatives[k]\n",
    "\n",
    "    return sum(false_negatives.values())\n",
    "\n",
    "def dup_lsh(buckets, signatures, lyrics, key, truth_key, silent=False):\n",
    "    tp, fp = 0, 0\n",
    "    fn = get_false_negatives(lyrics, TRUTH_KEY)\n",
    "    \n",
    "    tested = {}\n",
    "    \n",
    "    for i, bucket in enumerate(buckets):\n",
    "        for j in bucket:\n",
    "            for k in bucket:\n",
    "                if k <= j or (j, k) in tested or (k, j) in tested:\n",
    "                    continue\n",
    "                \n",
    "                same_key = lyrics[j][truth_key] == lyrics[k][truth_key]\n",
    "                \n",
    "                if same_key:\n",
    "                    tp += 1\n",
    "                    fn -= 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "                \n",
    "                tested[j, k] = True\n",
    "        \n",
    "        if not silent and i > 0 and i % (len(buckets) / 10) == 0:\n",
    "            print(\"%d of %d\" % (i, len(buckets)))\n",
    "\n",
    "    duplicates = tp + fp\n",
    "    precision = tp / float(tp + fp)\n",
    "    recall = tp / float(tp + fn)\n",
    "    \n",
    "    if not silent:\n",
    "        print(\"Done!\")\n",
    "        print(\"dup=%d, tp=%d, fp=%d, fn=%d, prec=%.2lf, rec=%.2lf\" %\n",
    "              (duplicates, tp, fp, fn, precision, recall))\n",
    "    \n",
    "    return duplicates, tp, fp, fn, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 of 431\n",
      "86 of 431\n",
      "129 of 431\n",
      "172 of 431\n",
      "215 of 431\n",
      "258 of 431\n",
      "301 of 431\n",
      "344 of 431\n",
      "387 of 431\n",
      "430 of 431\n",
      "Done!\n",
      "dup=117, tp=36, fp=81, fn=3, prec=0.31, rec=0.92\n",
      "CPU times: user 8.48 ms, sys: 141 µs, total: 8.62 ms\n",
      "Wall time: 6.61 ms\n"
     ]
    }
   ],
   "source": [
    "%time _ = dup_lsh(buckets, signatures, lyrics_test, key=KEY, truth_key=TRUTH_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search(lyrics, key, truth_key):\n",
    "    N_BUCKETS = 2 ** 64\n",
    "\n",
    "    for N_TOKENS in [5, 4, 6, 7, 8]:\n",
    "        for N_HASHES in [100, 200, 300]:\n",
    "            for N_BANDS in [12, 14, 16, 18, 20]:\n",
    "                print(\"N_TOKENS=%d, N_HASHES=%d, N_BANDS=%d, N_BUCKETS=%d\" % (N_TOKENS, N_HASHES,\n",
    "                                                                              N_BANDS, N_BUCKETS))\n",
    "                shingles = to_shingles(lyrics, n_tokens=N_TOKENS, key=KEY, silent=True)\n",
    "                signatures = to_signatures(shingles, n_hashes=N_HASHES, silent=True)\n",
    "                del shingles\n",
    "                buckets = lsh(signatures, n_bands=N_BANDS, n_buckets=N_BUCKETS, silent=True)\n",
    "                duplicates, tp, fp, fn, precision, recall = dup_lsh(buckets, signatures, lyrics,\n",
    "                                                                    key=KEY, truth_key=TRUTH_KEY, silent=True)\n",
    "                print(\"dup=%d, tp=%d, fp=%d, fn=%d, prec=%.2f, rec=%.2f\" %\n",
    "                      (duplicates, tp, fp, fn, precision, recall))\n",
    "\n",
    "%time grid_search(lyrics_train, key=KEY, truth_key=TRUTH_KEY)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
